{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 16:07:00,441 - INFO - Procesadas 24 secuencias. Guardadas en: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\cleaned_sequences.fasta\n",
      "2025-04-05 16:07:00,445 - INFO - Puedes abrir el archivo en un editor de texto: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\cleaned_sequences.fasta\n"
     ]
    }
   ],
   "source": [
    "# 01_preprocess.py\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import re\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Configuración de rutas\n",
    "INPUT_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\data\\secuencias.fasta.txt\")\n",
    "RESULTS_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\")\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configura el sistema de logging\"\"\"\n",
    "    RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(RESULTS_PATH / \"preprocess.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def clean_sequences(input_path: Path, output_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Limpia y filtra secuencias FASTA:\n",
    "    - Elimina caracteres no estándar\n",
    "    - Convierte a mayúsculas\n",
    "    - Filtra secuencias demasiado cortas\n",
    "    - Separa secuencias concatenadas con 'xxxxx'\n",
    "    \"\"\"\n",
    "    clean_seqs = []\n",
    "    \n",
    "    for record in SeqIO.parse(input_path, \"fasta\"):\n",
    "        # Procesar cada secuencia\n",
    "        seq = str(record.seq).upper()\n",
    "        \n",
    "        # Separar secuencias concatenadas con 'xxxxx'\n",
    "        sub_seqs = re.split(r'[XxNn]{4,}', seq)\n",
    "        \n",
    "        for i, sub_seq in enumerate(sub_seqs):\n",
    "            # Limpiar caracteres no estándar\n",
    "            clean_seq = re.sub(r'[^ATCG]', '', sub_seq)\n",
    "            \n",
    "            if len(clean_seq) >= 100:  # Filtro por longitud mínima\n",
    "                # Crear nuevo registro con ID único\n",
    "                new_id = f\"{record.id}_part{i+1}\" if len(sub_seqs) > 1 else record.id\n",
    "                clean_record = SeqRecord(\n",
    "                    Seq(clean_seq),\n",
    "                    id=new_id,\n",
    "                    description=f\"Cleaned version of {record.id}, length={len(clean_seq)}\"\n",
    "                )\n",
    "                clean_seqs.append(clean_record)\n",
    "    \n",
    "    # Guardar resultados en archivo FASTA normal (sin comprimir)\n",
    "    with open(output_path, 'w') as f:\n",
    "        SeqIO.write(clean_seqs, f, \"fasta\")\n",
    "    \n",
    "    logging.info(f\"Procesadas {len(clean_seqs)} secuencias. Guardadas en: {output_path}\")\n",
    "    logging.info(f\"Puedes abrir el archivo en un editor de texto: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    try:\n",
    "        output_file = RESULTS_PATH / \"cleaned_sequences.fasta\"\n",
    "        clean_sequences(INPUT_PATH, output_file)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en preprocesamiento: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#BLAST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 19:36:28,845 - INFO - Iniciando BLAST remoto con 24 secuencias\n",
      "2025-04-05 19:36:28,848 - INFO - Parámetros: word_size=7, gapcosts=5 2, evalue=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python_env\\proyecto_troncal2\\Lib\\site-packages\\Bio\\Blast\\NCBIWWW.py:281: BiopythonWarning: BLAST request Z3C3D4JM013 is taking longer than 10 minutes, consider re-issuing it\n",
      "  warnings.warn(\n",
      "2025-04-05 19:47:30,869 - INFO - Resultados guardados en: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\blast_results.xml\n",
      "2025-04-05 19:47:30,902 - INFO - Resultados detallados guardados en: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\blast_results_detailed.csv\n",
      "2025-04-05 19:47:30,910 - INFO - Resumen de resultados guardado en: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\blast_results_summary.csv\n",
      "2025-04-05 19:47:30,913 - INFO - \n",
      "Mejores hits encontrados:\n",
      "             evalue  bit_score  percent_identity  query_id  \\\n",
      "organism                                                     \n",
      "Desconocido     0.0    655.909         76.809584        51   \n",
      "\n",
      "                                                 hit_accession  \n",
      "organism                                                        \n",
      "Desconocido  KA652115, LA010820, LA010821, LA020528, LA0349...  \n"
     ]
    }
   ],
   "source": [
    "# 02_blast.py\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "# Configuración básica\n",
    "RESULTS_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\")\n",
    "DEFAULT_PARAMS = {\n",
    "    \"program\": \"blastn\",\n",
    "    \"database\": \"refseq_viral\",\n",
    "    \"e_value\": 1e-10,\n",
    "    \"hitlist_size\": 50,\n",
    "    \"word_size\": 11,\n",
    "    \"gapcosts\": \"5 2\"  # Solo parámetros soportados por qblast\n",
    "}\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configura el sistema de logging\"\"\"\n",
    "    RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(RESULTS_PATH / \"blast_analysis.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def run_remote_blast(\n",
    "    query_file: Path,\n",
    "    program: str = DEFAULT_PARAMS[\"program\"],\n",
    "    database: str = DEFAULT_PARAMS[\"database\"],\n",
    "    e_value: float = DEFAULT_PARAMS[\"e_value\"],\n",
    "    hitlist_size: int = DEFAULT_PARAMS[\"hitlist_size\"],\n",
    "    word_size: int = DEFAULT_PARAMS[\"word_size\"],\n",
    "    gapcosts: str = DEFAULT_PARAMS[\"gapcosts\"],\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Ejecuta BLAST remoto con parámetros personalizables\n",
    "    \n",
    "    Args:\n",
    "        query_file: Archivo FASTA con secuencias\n",
    "        program: blastn, blastp, etc.\n",
    "        database: Base de datos a usar\n",
    "        e_value: Umbral de significancia\n",
    "        hitlist_size: Número máximo de resultados\n",
    "        word_size: Tamaño de palabra (7-15 para blastn)\n",
    "        gapcosts: Costos de gaps (\"abrir extender\")\n",
    "        \n",
    "    Returns:\n",
    "        Ruta al archivo XML con resultados\n",
    "    \"\"\"\n",
    "    output_file = RESULTS_PATH / \"blast_results.xml\"\n",
    "    \n",
    "    records = list(SeqIO.parse(query_file, \"fasta\"))\n",
    "    if not records:\n",
    "        raise ValueError(\"No se encontraron secuencias en el archivo de consulta\")\n",
    "    \n",
    "    logging.info(f\"Iniciando BLAST remoto con {len(records)} secuencias\")\n",
    "    logging.info(f\"Parámetros: word_size={word_size}, gapcosts={gapcosts}, evalue={e_value}\")\n",
    "\n",
    "    with open(query_file) as f:\n",
    "        fasta_data = f.read()\n",
    "    \n",
    "    try:\n",
    "        # Solo parámetros soportados por NCBIWWW.qblast()\n",
    "        blast_params = {\n",
    "            \"program\": program,\n",
    "            \"database\": database,\n",
    "            \"sequence\": fasta_data,\n",
    "            \"expect\": e_value,\n",
    "            \"hitlist_size\": hitlist_size,\n",
    "            \"format_type\": \"XML\",\n",
    "            \"word_size\": word_size,\n",
    "            \"gapcosts\": gapcosts,\n",
    "        }\n",
    "        \n",
    "        handle = NCBIWWW.qblast(**blast_params)\n",
    "        \n",
    "        with open(output_file, \"w\") as f:\n",
    "            blast_results = handle.read()\n",
    "            f.write(blast_results)\n",
    "        \n",
    "        logging.info(f\"Resultados guardados en: {output_file}\")\n",
    "        return output_file\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en BLAST remoto: {e}\")\n",
    "        raise\n",
    "\n",
    "def parse_blast_results(xml_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Procesa resultados XML de BLAST\"\"\"\n",
    "    hits = []\n",
    "    \n",
    "    try:\n",
    "        with open(xml_path, 'r') as f:\n",
    "            blast_records = NCBIXML.parse(f)\n",
    "            \n",
    "            for record in blast_records:\n",
    "                if not record.alignments:\n",
    "                    continue\n",
    "                    \n",
    "                for align in record.alignments:\n",
    "                    for hsp in align.hsps:\n",
    "                        hit_def = align.hit_def\n",
    "                        organism = re.search(r'\\[(.*?)\\]', hit_def)\n",
    "                        organism = organism.group(1) if organism else \"Desconocido\"\n",
    "                        \n",
    "                        hits.append({\n",
    "                            \"query_id\": record.query,\n",
    "                            \"query_length\": record.query_length,\n",
    "                            \"hit_accession\": align.accession,\n",
    "                            \"hit_definition\": hit_def,\n",
    "                            \"organism\": organism,\n",
    "                            \"evalue\": hsp.expect,\n",
    "                            \"bit_score\": hsp.bits,\n",
    "                            \"alignment_length\": hsp.align_length,\n",
    "                            \"percent_identity\": hsp.identities / hsp.align_length * 100,\n",
    "                            \"query_start\": hsp.query_start,\n",
    "                            \"query_end\": hsp.query_end,\n",
    "                            \"hit_start\": hsp.sbjct_start,\n",
    "                            \"hit_end\": hsp.sbjct_end,\n",
    "                            \"alignment\": f\"{hsp.query[0:50]}...\" if len(hsp.query) > 50 else hsp.query\n",
    "                        })\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al parsear resultados: {str(e)}\")\n",
    "        raise\n",
    "    \n",
    "    df = pd.DataFrame(hits)\n",
    "    \n",
    "    if not df.empty:\n",
    "        df = df.sort_values([\"evalue\", \"bit_score\"], ascending=[True, False])\n",
    "        df = df.drop_duplicates(subset=[\"query_id\", \"hit_accession\"], keep=\"first\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def save_results(df: pd.DataFrame, params: dict):\n",
    "    \"\"\"Guarda los resultados en archivos CSV\"\"\"\n",
    "    csv_path = RESULTS_PATH / \"blast_results_detailed.csv\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    logging.info(f\"Resultados detallados guardados en: {csv_path}\")\n",
    "    \n",
    "    if not df.empty:\n",
    "        summary = df.groupby(\"organism\").agg({\n",
    "            \"evalue\": \"min\",\n",
    "            \"bit_score\": \"max\",\n",
    "            \"percent_identity\": \"mean\",\n",
    "            \"query_id\": \"count\",\n",
    "            \"hit_accession\": lambda x: \", \".join(sorted(set(x)))\n",
    "        }).sort_values([\"evalue\", \"bit_score\"], ascending=[True, False])\n",
    "        \n",
    "        summary_path = RESULTS_PATH / \"blast_results_summary.csv\"\n",
    "        summary.to_csv(summary_path)\n",
    "        \n",
    "        params_path = RESULTS_PATH / \"blast_parameters.txt\"\n",
    "        with open(params_path, 'w') as f:\n",
    "            for key, value in params.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "        \n",
    "        logging.info(f\"Resumen de resultados guardado en: {summary_path}\")\n",
    "        logging.info(\"\\nMejores hits encontrados:\\n\" + str(summary.head(10)))\n",
    "    else:\n",
    "        logging.warning(\"No se encontraron hits significativos\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    \n",
    "    try:\n",
    "        query_seq = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\cleaned_sequences.fasta\")\n",
    "        \n",
    "        if not query_seq.exists():\n",
    "            raise FileNotFoundError(f\"No se encontró el archivo de consulta: {query_seq}\")\n",
    "        \n",
    "        # Parámetros personalizados (solo los soportados por qblast)\n",
    "        custom_params = {\n",
    "            \"program\": \"blastn\",\n",
    "            \"database\": \"env_nt\",\n",
    "            \"e_value\": 1e-3,\n",
    "            \"hitlist_size\": 50,\n",
    "            \"word_size\": 7,  # Ajustable entre 7-15\n",
    "            \"gapcosts\": \"5 2\"  # Costos de gaps\n",
    "        }\n",
    "        \n",
    "        blast_results = run_remote_blast(query_seq, **custom_params)\n",
    "        df = parse_blast_results(blast_results)\n",
    "        save_results(df, custom_params)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inesperado: {str(e)}\", exc_info=True)\n",
    "\n",
    "\n",
    "        # Añade esto después de obtener los resultados vacíos\n",
    "if df.empty:\n",
    "    logging.warning(r\"\"\"\n",
    "    No se encontraron hits significativos. Recomendaciones:\n",
    "    1. Aumentar el e_value \n",
    "    2. Reducir el word_size \n",
    "    3. Cambiar la base de datos (Ver C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\Bases_de_datos_blast.xlsx )\n",
    "    4. Verificar la calidad de las secuencias de entrada\n",
    "    5. Considerar usar blastx si son secuencias codificantes\n",
    "    \"\"\")\n",
    "    \n",
    "    # Generar archivo con recomendaciones\n",
    "    with open(RESULTS_PATH / \"recommendations.txt\", \"w\") as f:\n",
    "        f.write(\"Recomendaciones para mejorar los resultados:\\n\")\n",
    "        f.write(f\"1. Aumentar e_value: Actual {custom_params['e_value']} -> Prueba con 1e-3\\n\")\n",
    "        f.write(f\"2. Reducir word_size: Actual {custom_params['word_size']} -> Prueba con 7\\n\")\n",
    "        f.write(\"3. Cambiar database: Actual 'refseq_viral' -> Prueba con 'nr' o 'nt'\\n\")\n",
    "        f.write(\"4. Verificar secuencias de entrada (contaminación, calidad)\\n\")\n",
    "        f.write(\"5. Para secuencias codificantes, considerar usar blastx\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#MULTIPLE ALIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-05 20:20:04,360 - INFO - Procesando archivo: C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\\cleaned_sequences.fasta\n",
      "2025-04-05 20:20:04,362 - INFO - Enviando secuencias a Clustal Omega remoto...\n",
      "2025-04-05 20:20:05,622 - ERROR - Error en alineamiento remoto: Error 400: <?xml version='1.0' encoding='UTF-8'?>\n",
      "<error>\n",
      " <description>Invalid parameters: \n",
      "Sequence -> Duplicated ID: unknown. Two sequences cannot share the same identifier</description>\n",
      "</error>\n",
      "Posibles causas:\n",
      "- Formato FASTA incorrecto\n",
      "- Secuencias demasiado largas\n",
      "- Caracteres inválidos en las secuencias\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\fgarc\\AppData\\Local\\Temp\\ipykernel_18448\\2831071403.py\", line 130, in <module>\n",
      "    aln_file = run_remote_clustal(input_seqs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\fgarc\\AppData\\Local\\Temp\\ipykernel_18448\\2831071403.py\", line 77, in run_remote_clustal\n",
      "    raise RuntimeError(error_msg)\n",
      "RuntimeError: Error 400: <?xml version='1.0' encoding='UTF-8'?>\n",
      "<error>\n",
      " <description>Invalid parameters: \n",
      "Sequence -> Duplicated ID: unknown. Two sequences cannot share the same identifier</description>\n",
      "</error>\n",
      "Posibles causas:\n",
      "- Formato FASTA incorrecto\n",
      "- Secuencias demasiado largas\n",
      "- Caracteres inválidos en las secuencias\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import time\n",
    "import os\n",
    "\n",
    "RESULTS_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\")\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configura el sistema de logging\"\"\"\n",
    "    RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(RESULTS_PATH / \"clustal_remote.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def validate_fasta(fasta_data: str) -> bool:\n",
    "    \"\"\"Valida el formato básico del archivo FASTA\"\"\"\n",
    "    lines = fasta_data.split('\\n')\n",
    "    if not lines[0].startswith('>'):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def run_remote_clustal(input_fasta: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Ejecuta Clustal Omega remoto usando la API de EMBL-EBI.\n",
    "    \n",
    "    Args:\n",
    "        input_fasta: Archivo FASTA con secuencias a alinear.\n",
    "        \n",
    "    Returns:\n",
    "        Ruta al archivo de alineamiento en formato CLUSTAL.\n",
    "    \"\"\"\n",
    "    output_aln = RESULTS_PATH / \"remote_alignment.clustal\"\n",
    "    \n",
    "    # Leer y validar el archivo FASTA\n",
    "    with open(input_fasta, 'r') as f:\n",
    "        fasta_data = f.read().strip()\n",
    "    \n",
    "    if not validate_fasta(fasta_data):\n",
    "        raise ValueError(\"Formato FASTA inválido. El archivo debe comenzar con un encabezado '>'\")\n",
    "    \n",
    "    # Verificar tamaño del archivo (límite ~1MB)\n",
    "    file_size = os.path.getsize(input_fasta) / 1024  # KB\n",
    "    if file_size > 1024:\n",
    "        raise ValueError(f\"El archivo es demasiado grande ({file_size:.2f} KB). Límite: 1024 KB\")\n",
    "    \n",
    "    # Configurar la solicitud a la API\n",
    "    url = \"https://www.ebi.ac.uk/Tools/services/rest/clustalo/run\"\n",
    "    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    data = {\n",
    "        \"email\": \"fgarciao2@correo.uss.cl\",  # Obligatorio\n",
    "        \"sequence\": fasta_data,\n",
    "        \"outfmt\": \"clustal\",\n",
    "        \"stype\": \"dna\",  # Especifica que son secuencias de ADN\n",
    "        \"dealign\": \"true\"  # Para realinear secuencias previamente alineadas\n",
    "    }\n",
    "    \n",
    "    logging.info(\"Enviando secuencias a Clustal Omega remoto...\")\n",
    "    \n",
    "    try:\n",
    "        # Paso 1: Enviar el trabajo\n",
    "        response = requests.post(url, headers=headers, data=data)\n",
    "        \n",
    "        # Verificar respuesta detallada\n",
    "        if response.status_code != 200:\n",
    "            error_msg = f\"Error {response.status_code}: {response.text}\"\n",
    "            if response.status_code == 400:\n",
    "                error_msg += \"\\nPosibles causas:\\n\"\n",
    "                error_msg += \"- Formato FASTA incorrecto\\n\"\n",
    "                error_msg += \"- Secuencias demasiado largas\\n\"\n",
    "                error_msg += \"- Caracteres inválidos en las secuencias\"\n",
    "            raise RuntimeError(error_msg)\n",
    "            \n",
    "        job_id = response.text\n",
    "        logging.info(f\"Job ID recibido: {job_id}\")\n",
    "        \n",
    "        # Paso 2: Verificar estado del trabajo\n",
    "        status_url = f\"https://www.ebi.ac.uk/Tools/services/rest/clustalo/status/{job_id}\"\n",
    "        start_time = time.time()\n",
    "        timeout = 600  # 10 minutos de timeout\n",
    "        \n",
    "        while True:\n",
    "            if time.time() - start_time > timeout:\n",
    "                raise TimeoutError(\"Tiempo de espera excedido para el alineamiento\")\n",
    "                \n",
    "            status_response = requests.get(status_url)\n",
    "            status = status_response.text\n",
    "            logging.info(f\"Estado actual: {status}\")\n",
    "            \n",
    "            if status == \"FINISHED\":\n",
    "                break\n",
    "            elif status in (\"FAILED\", \"ERROR\"):\n",
    "                raise RuntimeError(f\"Error en Clustal Omega: {status}\")\n",
    "                \n",
    "            time.sleep(10)  # Esperar 10 segundos entre verificaciones\n",
    "        \n",
    "        # Paso 3: Obtener resultados\n",
    "        result_url = f\"https://www.ebi.ac.uk/Tools/services/rest/clustalo/result/{job_id}/clustal\"\n",
    "        result_response = requests.get(result_url)\n",
    "        result_response.raise_for_status()\n",
    "        \n",
    "        # Guardar el alineamiento\n",
    "        with open(output_aln, 'w') as f:\n",
    "            f.write(result_response.text)\n",
    "        \n",
    "        logging.info(f\"Alineamiento guardado en: {output_aln}\")\n",
    "        return output_aln\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"Error en la conexión: {str(e)}\")\n",
    "        if hasattr(e, 'response') and e.response is not None:\n",
    "            logging.error(f\"Respuesta del servidor: {e.response.text}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    try:\n",
    "        # Corregir la ruta del archivo de entrada\n",
    "        input_seqs = RESULTS_PATH / \"cleaned_sequences.fasta\"\n",
    "        \n",
    "        if not input_seqs.exists():\n",
    "            raise FileNotFoundError(f\"No se encontró el archivo FASTA: {input_seqs}\")\n",
    "        \n",
    "        logging.info(f\"Procesando archivo: {input_seqs}\")\n",
    "        aln_file = run_remote_clustal(input_seqs)\n",
    "        logging.info(\"¡Alineamiento completado con éxito!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en alineamiento remoto: {str(e)}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#VISUALIZACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 03_visualize.py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
    "from Bio.Phylo import draw\n",
    "from Bio import AlignIO, Phylo\n",
    "import pylab\n",
    "\n",
    "# Configuración\n",
    "RESULTS_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\")\n",
    "PLOT_STYLE = \"seaborn-v0_8-poster\"\n",
    "COLOR_PALETTE = \"viridis\"\n",
    "\n",
    "def setup_logging():\n",
    "    RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(RESULTS_PATH / \"visualization.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def plot_blast_results():\n",
    "    \"\"\"Genera múltiples visualizaciones de los resultados BLAST\"\"\"\n",
    "    # Configurar estilo\n",
    "    plt.style.use(PLOT_STYLE)\n",
    "    sns.set_palette(COLOR_PALETTE)\n",
    "    \n",
    "    # Cargar datos\n",
    "    csv_path = RESULTS_PATH / \"blast_results_detailed.csv\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    if df.empty:\n",
    "        logging.warning(\"No hay datos para visualizar\")\n",
    "        return []\n",
    "    \n",
    "    # Filtrar los mejores hits por query\n",
    "    top_hits = df.loc[df.groupby(\"query_id\")[\"evalue\"].idxmin()]\n",
    "    \n",
    "    # 1. Gráfico de los mejores hits por E-value\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_organisms = top_hits.nsmallest(20, \"evalue\")\n",
    "    # Calcular -log10(evalue) para mejor visualización\n",
    "    top_organisms[\"-log_evalue\"] = -np.log10(top_organisms[\"evalue\"].astype(float) + 1e-300)  # Evitar log(0)\n",
    "    sns.barplot(\n",
    "        data=top_organisms,\n",
    "        y=\"organism\",\n",
    "        x=\"-log_evalue\",\n",
    "        estimator=np.median,\n",
    "        errorbar=None\n",
    "    )\n",
    "    plt.title(\"Top 20 Organismos por E-value (mejores hits)\")\n",
    "    plt.xlabel(\"-log10(E-value)\")\n",
    "    plt.ylabel(\"Organismo\")\n",
    "    plt.tight_layout()\n",
    "    plot1 = RESULTS_PATH / \"blast_top_organisms.png\"\n",
    "    plt.savefig(plot1, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 2. Heatmap de identidad de secuencia\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pivot_data = top_hits.pivot_table(\n",
    "        index=\"organism\",\n",
    "        columns=\"query_id\",\n",
    "        values=\"percent_identity\",\n",
    "        aggfunc=\"mean\"\n",
    "    ).fillna(0)\n",
    "    sns.heatmap(\n",
    "        pivot_data,\n",
    "        cmap=\"YlOrRd\",\n",
    "        annot=True,\n",
    "        fmt=\".1f\",\n",
    "        linewidths=.5\n",
    "    )\n",
    "    plt.title(\"Porcentaje de Identidad por Organismo y Query\")\n",
    "    plt.tight_layout()\n",
    "    plot2 = RESULTS_PATH / \"blast_identity_heatmap.png\"\n",
    "    plt.savefig(plot2, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3. Gráfico de distribución de E-values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(\n",
    "        data=top_hits,\n",
    "        x=\"evalue\",\n",
    "        bins=50,\n",
    "        log_scale=True\n",
    "    )\n",
    "    plt.title(\"Distribución de E-values de los mejores hits\")\n",
    "    plt.xlabel(\"E-value (log scale)\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plot3 = RESULTS_PATH / \"blast_evalue_distribution.png\"\n",
    "    plt.savefig(plot3, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    logging.info(f\"Gráficos guardados en:\\n- {plot1}\\n- {plot2}\\n- {plot3}\")\n",
    "    return [plot1, plot2, plot3]\n",
    "\n",
    "def generate_phylogenetic_tree():\n",
    "    \"\"\"Genera un árbol filogenético basado en los alineamientos\"\"\"\n",
    "    try:\n",
    "        # Esto requeriría un archivo de alineamiento múltiple (ej. de CLUSTAL)\n",
    "        aln_file = RESULTS_PATH / \"alignment.clustal\"\n",
    "        \n",
    "        if not aln_file.exists():\n",
    "            logging.warning(\"Archivo de alineamiento no encontrado. Ejecuta CLUSTAL primero.\")\n",
    "            return None\n",
    "            \n",
    "        alignment = AlignIO.read(aln_file, \"clustal\")\n",
    "        \n",
    "        # Calcular matriz de distancia\n",
    "        calculator = DistanceTreeConstructor()\n",
    "        dm = calculator.get_distance(alignment)\n",
    "        \n",
    "        # Construir árbol (método UPGMA)\n",
    "        tree = calculator.upgma(dm)\n",
    "        \n",
    "        # Dibujar árbol\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        Phylo.draw(tree, do_show=False)\n",
    "        plt.title(\"Árbol Filogenético basado en alineamiento múltiple\")\n",
    "        \n",
    "        plot_path = RESULTS_PATH / \"phylogenetic_tree.png\"\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        logging.info(f\"Árbol filogenético guardado en: {plot_path}\")\n",
    "        return plot_path\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error al generar árbol filogenético: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    try:\n",
    "        plots = plot_blast_results()\n",
    "        tree_plot = generate_phylogenetic_tree()\n",
    "        \n",
    "        if tree_plot:\n",
    "            plots.append(tree_plot)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en visualización: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#ANNOTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez, SeqIO\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Configuración\n",
    "Entrez.email = \"tu.email@institucion.edu\"  # Obligatorio para usar NCBI\n",
    "RESULTS_PATH = Path(r\"C:\\Users\\fgarc\\OneDrive\\Escritorio\\Doctorado\\Ramos\\1° Semestre\\Troncal\\proyecto_troncal2\\results\\2025-03-30_FG\\identificacion_patogeno\")\n",
    "NCBI_DB = \"nucleotide\"\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5  # segundos\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configura el sistema de logging\"\"\"\n",
    "    RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        handlers=[\n",
    "            logging.FileHandler(RESULTS_PATH / \"annotation.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def fetch_genome(accession: str) -> Path:\n",
    "    \"\"\"\n",
    "    Descarga un genoma completo desde NCBI\n",
    "    \n",
    "    Args:\n",
    "        accession: Número de acceso del genoma\n",
    "        \n",
    "    Returns:\n",
    "        Ruta al archivo FASTA descargado\n",
    "    \"\"\"\n",
    "    output_file = RESULTS_PATH / f\"{accession}.fasta\"\n",
    "    \n",
    "    if output_file.exists():\n",
    "        logging.info(f\"Archivo ya existe: {output_file}\")\n",
    "        return output_file\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            logging.info(f\"Descargando genoma {accession} (intento {attempt+1})...\")\n",
    "            \n",
    "            # Obtener el genoma\n",
    "            handle = Entrez.efetch(\n",
    "                db=NCBI_DB,\n",
    "                id=accession,\n",
    "                rettype=\"fasta\",\n",
    "                retmode=\"text\"\n",
    "            )\n",
    "            \n",
    "            # Guardar en archivo\n",
    "            with open(output_file, \"w\") as f:\n",
    "                f.write(handle.read())\n",
    "            \n",
    "            handle.close()\n",
    "            logging.info(f\"Genoma descargado en: {output_file}\")\n",
    "            return output_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Error en intento {attempt+1}: {e}\")\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                time.sleep(RETRY_DELAY)\n",
    "    \n",
    "    raise RuntimeError(f\"No se pudo descargar el genoma {accession} después de {MAX_RETRIES} intentos\")\n",
    "\n",
    "def annotate_genome(fasta_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Realiza una anotación básica del genoma usando NCBI\n",
    "    \n",
    "    Args:\n",
    "        fasta_file: Archivo FASTA con el genoma\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame con las características anotadas\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Leer el genoma\n",
    "        record = next(SeqIO.parse(fasta_file, \"fasta\"))\n",
    "        \n",
    "        # Obtener anotación desde NCBI\n",
    "        handle = Entrez.efetch(\n",
    "            db=NCBI_DB,\n",
    "            id=record.id,\n",
    "            rettype=\"gb\",\n",
    "            retmode=\"text\"\n",
    "        )\n",
    "        \n",
    "        gb_record = SeqIO.read(handle, \"genbank\")\n",
    "        handle.close()\n",
    "        \n",
    "        # Extraer características\n",
    "        features = []\n",
    "        for feature in gb_record.features:\n",
    "            if feature.type not in [\"source\", \"gene\"]:\n",
    "                continue\n",
    "                \n",
    "            qualifiers = feature.qualifiers\n",
    "            start = feature.location.start\n",
    "            end = feature.location.end\n",
    "            strand = feature.location.strand\n",
    "            \n",
    "            features.append({\n",
    "                \"type\": feature.type,\n",
    "                \"start\": start,\n",
    "                \"end\": end,\n",
    "                \"strand\": strand,\n",
    "                \"product\": qualifiers.get(\"product\", [\"\"])[0],\n",
    "                \"gene\": qualifiers.get(\"gene\", [\"\"])[0],\n",
    "                \"protein_id\": qualifiers.get(\"protein_id\", [\"\"])[0],\n",
    "                \"note\": qualifiers.get(\"note\", [\"\"])[0]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(features)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en anotación: {e}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_logging()\n",
    "    try:\n",
    "        # Obtener el mejor hit de BLAST\n",
    "        blast_summary = pd.read_csv(RESULTS_PATH / \"blast_results_summary.csv\")\n",
    "        best_hit = blast_summary.iloc[0]\n",
    "        \n",
    "        # Extraer número de acceso (simplificado)\n",
    "        accession = best_hit[\"accessions\"].split(\",\")[0].strip()\n",
    "        logging.info(f\"Mejor hit encontrado: {accession}\")\n",
    "        \n",
    "        # Descargar genoma\n",
    "        genome_file = fetch_genome(accession)\n",
    "        \n",
    "        # Anotar genoma\n",
    "        annotations = annotate_genome(genome_file)\n",
    "        \n",
    "        # Guardar anotaciones\n",
    "        anno_path = RESULTS_PATH / f\"{accession}_annotations.csv\"\n",
    "        annotations.to_csv(anno_path, index=False)\n",
    "        logging.info(f\"Anotaciones guardadas en: {anno_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error en anotación genómica: {e}\", exc_info=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (poryecto_troncal2)",
   "language": "python",
   "name": "proyecto_troncal2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
